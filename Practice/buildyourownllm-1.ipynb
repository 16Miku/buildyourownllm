{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# simplemodel.py"
      ],
      "metadata": {
        "id": "HBPJ-BRyjbQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rGlSe8KvaSHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "863d68b2-ed6a-48e2-c5bd-1b85f8ad218e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "春江月 张先生疑被。\n",
            "\n",
            "倦旅。\n",
            "清歌声月边、莼鲈清唱，尽一卮酒红蕖花月，彩笼里繁蕊珠玑。\n",
            "只今古。\n",
            "浣溪月上宾鸿相照。\n",
            "乞团，烟渚澜翻覆古1\n",
            "半吐，还在蓬瀛烟沼。\n",
            "木兰花露弓刀，更任东南楼缥缈。\n",
            "黄柳，\n"
          ]
        }
      ],
      "source": [
        "# simplemodel.py\n",
        "import random # 导入random模块，用于生成随机数\n",
        "\n",
        "random.seed(42) # 设置随机数种子，确保每次运行结果一致，便于调试和复现。如果注释掉此行，每次运行将得到不同的随机结果。\n",
        "\n",
        "prompt = \"春江\" # 定义初始的生成文本，我们将从“春江”开始生成后续的词语。\n",
        "max_new_token = 100 # 定义模型将生成的新词语的最大数量。\n",
        "\n",
        "# 打开并读取ci.txt文件。\n",
        "# 'r' 表示读取模式，encoding='utf-8' 指定文件编码为UTF-8，以正确处理中文字符。\n",
        "with open('ci.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read() # 将整个文件内容读取到一个字符串变量text中。\n",
        "\n",
        "chars = sorted(list(set(text))) # 从text中提取所有不重复的字符，并进行排序，形成我们的词汇表（chars）。\n",
        "vocab_size = len(chars) # 计算词汇表的大小，即不重复字符的数量。\n",
        "\n",
        "# 创建字符到整数的映射 (string to integer)。\n",
        "# stoi是一个字典，键是字符，值是该字符在词汇表中的索引（ID）。\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "# 创建整数到字符的映射 (integer to string)。\n",
        "# itos是一个字典，键是字符的索引（ID），值是对应的字符。\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# 定义一个编码函数，将字符串转换为整数ID列表。\n",
        "# 例如，encode(\"春江\") -> [stoi['春'], stoi['江']]\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "# 定义一个解码函数，将整数ID列表转换为字符串。\n",
        "# 例如，decode([stoi['春'], stoi['江']]) -> \"春江\"\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# 初始化一个二维列表（矩阵），用于存储字符之间的转换频率。\n",
        "# transition[i][j] 将表示在字符i后面出现字符j的次数。\n",
        "# 矩阵的大小是 vocab_size * vocab_size，所有初始值为0。\n",
        "transition = [[0 for _ in range(vocab_size)] for _ in range(vocab_size)]\n",
        "\n",
        "# 遍历文本，统计字符间的转换频率。\n",
        "# 从文本的第一个字符到倒数第二个字符，因为我们需要看当前字符和下一个字符。\n",
        "for i in range(len(text) - 1):\n",
        "    current_token_id = encode(text[i])[0] # 获取当前字符的ID。由于encode返回列表，我们取第一个元素。\n",
        "    next_token_id = encode(text[i + 1])[0] # 获取下一个字符的ID。\n",
        "    transition[current_token_id][next_token_id] += 1 # 在转换矩阵中，将当前字符ID到下一个字符ID的频率加1。\n",
        "\n",
        "# 初始化生成序列，从prompt开始。\n",
        "generated_token = encode(prompt)\n",
        "\n",
        "# 开始生成新的词语。循环 max_new_token - 1 次，因为prompt已经有一个词了，我们还需要生成 max_new_token - 1 个。\n",
        "for i in range(max_new_token - 1):\n",
        "    current_token_id = generated_token[-1] # 获取当前已生成序列中的最后一个字符的ID。\n",
        "    logits = transition[current_token_id] # 从转换矩阵中获取当前字符后面所有字符的出现频率（得分）。\n",
        "    total = sum(logits) # 计算所有频率的总和，用于后续的归一化。\n",
        "\n",
        "    # 归一化处理：将频率转换为概率。\n",
        "    # 如果total为0（即当前字符从未在训练数据中出现过），则所有概率都为0，这将导致random.choices出错。\n",
        "    # 这里是一个简单的处理，如果total为0，logits将全是0，random.choices会报错。\n",
        "    # 实际应用中需要更健壮的处理，例如添加平滑项或确保total不为0。\n",
        "    logits = [logit / total for logit in logits]\n",
        "\n",
        "    # 根据计算出的概率分布随机选择下一个字符的ID。\n",
        "    # range(vocab_size) 是所有可能的字符ID。\n",
        "    # weights=logits 提供每个字符ID被选择的概率。\n",
        "    # k=1 表示只选择一个字符。\n",
        "    next_token_id = random.choices(range(vocab_size), weights=logits, k=1)[0]\n",
        "    generated_token.append(next_token_id) # 将选择的下一个字符ID添加到生成序列中。\n",
        "    current_token_id = next_token_id # 更新 current_token_id 为新生成的字符ID，为下一次循环做准备。\n",
        "\n",
        "print(decode(generated_token)) # 将最终生成的整数ID序列解码为可读的字符串并打印。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N-tArh1-jxFU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_16d2xsUkAab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nSN5ggUykAUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  重构：更具“机器学习风格”的Bigram模型\n",
        "# 为了更好地理解后续真实的 PyTorch 代码，我们将对 simplemodel.py 进行重构，使其更符合机器学习的编程范式。有 PyTorch 背景的同学可以快速浏览本节。\n",
        "\n",
        "# 主要变化：\n",
        "\n",
        "# 类封装：将 Tokenizer 和 BigramLanguageModel 封装成类。\n",
        "\n",
        "# 批处理：引入 batch_size 和 block_size 概念，实现批处理数据加载和模型推理，为 GPU 并行计算做准备。\n",
        "\n",
        "# `forward` 和 `generate` 方法：模仿深度学习框架中模型的 forward 方法（用于计算输出）和 generate 方法（用于序列生成）。"
      ],
      "metadata": {
        "id": "acezs7E9kAJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simplebigrammodel.py"
      ],
      "metadata": {
        "id": "HSNyvWz4jIZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simplebigrammodel.py\n",
        "import random # 导入random模块，用于生成随机数和进行随机采样\n",
        "from typing import List # 从typing模块导入List，用于类型注解，提高代码可读性和可维护性\n",
        "import time\n",
        "\n",
        "random.seed(42) # 设置随机数种子，确保每次运行结果一致，便于调试和复现。\n",
        "\n",
        "# 定义全局参数\n",
        "prompts = [\"春江\", \"往事\"] # 定义多个初始的生成文本，我们将从这些prompt开始生成。\n",
        "max_new_token = 100 # 定义模型将生成的新词语的最大数量。\n",
        "max_iters = 8000 # 定义“训练”的最大迭代次数。这里并不是真正的训练，只是多次统计频率，但模拟了训练循环。\n",
        "batch_size = 32 # 定义每个批次（batch）的大小，即每次处理多少个独立的序列。\n",
        "block_size = 8 # 定义每个序列的最大长度（也称为上下文长度或块大小），即模型在做预测时会考虑前面多少个字符。\n",
        "\n",
        "# 打开并读取ci.txt文件，与之前相同。\n",
        "with open('ci.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenizer 类：封装词汇表的编码和解码逻辑\n",
        "class Tokenizer:\n",
        "    def __init__(self, text: str):\n",
        "        # 构造函数，初始化Tokenizer时需要传入整个文本数据\n",
        "        self.chars = sorted(list(set(text))) # 从文本中提取所有不重复的字符，并排序，作为词汇表。\n",
        "        self.vocab_size = len(self.chars) # 计算词汇表的大小。\n",
        "        self.stoi = {ch: i for i, ch in enumerate(self.chars)} # 字符到ID的映射字典。\n",
        "        self.itos = {i: ch for i, ch in enumerate(self.chars)} # ID到字符的映射字典。\n",
        "\n",
        "    def encode(self, s: str) -> List[int]:\n",
        "        # 将字符串编码为整数ID列表。\n",
        "        return [self.stoi[c] for c in s]\n",
        "\n",
        "    def decode(self, l: List[int]) -> str:\n",
        "        # 将整数ID列表解码为字符串。\n",
        "        return ''.join([self.itos[i] for i in l])\n",
        "\n",
        "# BigramLanguageModel 类：模拟机器学习模型结构\n",
        "class BigramLanguageModel():\n",
        "    def __init__(self, vocab_size: int):\n",
        "        # 构造函数，初始化模型时需要传入词汇表大小。\n",
        "        self.vocab_size = vocab_size\n",
        "        # 初始化转换矩阵。\n",
        "        # transition[i][j] 表示字符i后面出现字符j的频率。\n",
        "        # 这是一个 vocab_size * vocab_size 的二维列表，所有初始值为0。\n",
        "        self.transition = [[0 for _ in range(vocab_size)] for _ in range(vocab_size)]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # 这是一个Python的特殊方法，使得类的实例可以直接像函数一样被调用，例如 model(x)。\n",
        "        # 它会内部调用forward方法，模仿PyTorch中nn.Module的行为。\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, idx: List[List[int]]) -> List[List[List[float]]]:\n",
        "        '''\n",
        "        模型的“前向传播”方法。\n",
        "        输入idx：一个二维数组，形状为 (B, T)。\n",
        "            B (Batch Size) 代表批次大小，即同时推理的序列数量。\n",
        "            T (Sequence Length) 代表每个序列的长度。\n",
        "            例如：[[1, 2, 3], [4, 5, 6]] 表示同时有2个序列，每个序列长度为3。\n",
        "        输出logits：一个三维数组，形状为 (B, T, vocab_size)。\n",
        "            它表示在每个批次中的每个token位置，预测下一个token是词汇表中各个token的“得分”（频率）。\n",
        "            例如：[[[0.1, 0.2, ...], [0.4, 0.5, ...], ...], ...]\n",
        "        '''\n",
        "        B = len(idx) # 获取批次大小。\n",
        "        T = len(idx[0]) # 获取每个序列的长度。\n",
        "\n",
        "        # 初始化一个三维列表来存储logits。\n",
        "        # logits[b][t] 将是一个长度为 vocab_size 的列表，表示在第b个批次的第t个token后面，\n",
        "        # 词汇表中每个token的出现频率。\n",
        "        logits = [\n",
        "            [[0.0 for _ in range(self.vocab_size)] # 内部列表，表示一个token后面所有可能token的频率\n",
        "             for _ in range(T)] # 中间列表，表示一个序列中所有token的频率\n",
        "            for _ in range(B) # 外部列表，表示所有批次的频率\n",
        "        ]\n",
        "\n",
        "        # 遍历批次和序列中的每个token，计算其下一个token的频率。\n",
        "        for b in range(B): # 遍历每个批次\n",
        "            for t in range(T): # 遍历当前批次中的每个token\n",
        "                current_token = idx[b][t] # 获取当前token的ID。\n",
        "                # 从预先统计的transition矩阵中获取当前token后面所有可能token的频率。\n",
        "                # 这一行是Bigram模型的核心：下一个token的预测只依赖于当前token。\n",
        "                logits[b][t] = self.transition[current_token]\n",
        "\n",
        "        return logits # 返回计算出的logits。\n",
        "\n",
        "    def generate(self, idx: List[List[int]], max_new_tokens: int) -> List[List[int]]:\n",
        "        '''\n",
        "        序列生成方法。\n",
        "        输入idx：一个二维数组，形状为 (B, T)，表示初始的prompt序列批次。\n",
        "        max_new_tokens：需要生成的新token的最大数量。\n",
        "        输出：一个二维数组，形状为 (B, T + max_new_tokens)，包含原始prompt和生成的新token。\n",
        "        '''\n",
        "        for _ in range(max_new_tokens): # 循环生成 max_new_tokens 个新token。\n",
        "            # 调用模型的forward方法，获取当前序列批次的logits。\n",
        "            # logits_batch 的形状是 (B, T_current, vocab_size)，其中 T_current 是当前序列的长度。\n",
        "            logits_batch = self(idx)\n",
        "\n",
        "            # 遍历每个批次中的序列，进行采样和扩展。\n",
        "            for batch_idx, logits_per_sequence in enumerate(logits_batch):\n",
        "                # 在Bigram模型中，我们只需要最后一个token的下一个token的概率来进行预测。\n",
        "                # logits_per_sequence 是当前批次中一个序列的logits，形状是 (T_current, vocab_size)。\n",
        "                # logits_per_sequence[-1] 提取了该序列中最后一个token的logits，形状是 (vocab_size)。\n",
        "                logits_of_last_token = logits_per_sequence[-1]\n",
        "\n",
        "                # 计算总频率，用于归一化。使用max(sum(logits), 1)防止除以零的情况。\n",
        "                total = max(sum(logits_of_last_token), 1)\n",
        "                # 归一化：将频率转换为概率。\n",
        "                probs_of_last_token = [logit / total for logit in logits_of_last_token]\n",
        "\n",
        "                # 根据概率分布随机采样下一个token的ID。\n",
        "                next_token = random.choices(\n",
        "                    range(self.vocab_size), # 所有可能的token ID\n",
        "                    weights=probs_of_last_token, # 对应的概率分布\n",
        "                    k=1 # 采样一个token\n",
        "                )[0] # random.choices返回一个列表，我们取第一个元素\n",
        "\n",
        "                # 将新生成的token添加到当前批次的序列中。\n",
        "                idx[batch_idx].append(next_token)\n",
        "        return idx # 返回包含生成token的完整序列批次。\n",
        "\n",
        "# 定义一个辅助函数，用于从整个数据集中随机获取一批数据\n",
        "def get_batch(tokens: List[int], batch_size: int, block_size: int) -> tuple[List[List[int]], List[List[int]]]:\n",
        "    '''\n",
        "    随机获取一批数据x和y用于“训练”。\n",
        "    x和y都是二维数组，可以用于并行处理。\n",
        "    其中y数组内的每一个值，都是x数组内对应位置的值的下一个值。\n",
        "    这种输入-输出对的设计是机器学习监督学习的常见模式。\n",
        "\n",
        "    例如：\n",
        "    如果原始tokens是 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "    选择一个随机起始索引 i = 0\n",
        "    x = [1, 2, 3] (block_size=3)\n",
        "    y = [2, 3, 4]\n",
        "\n",
        "    选择另一个随机起始索引 j = 7\n",
        "    x = [8, 9, 10]\n",
        "    y = [9, 10, 11] (这里假设原始tokens足够长)\n",
        "    '''\n",
        "    # 随机选择 batch_size 个起始索引。\n",
        "    # range(len(tokens) - block_size) 确保选取的起始索引有足够的后续token来构成一个完整的 block_size 序列。\n",
        "    ix = random.choices(range(len(tokens) - block_size), k=batch_size)\n",
        "\n",
        "    x, y = [], [] # 初始化输入序列列表x和目标序列列表y\n",
        "    for i in ix: # 遍历每个随机选取的起始索引\n",
        "        x.append(tokens[i:i+block_size]) # 从原始token序列中截取一个长度为 block_size 的子序列作为输入x\n",
        "        y.append(tokens[i+1:i+block_size+1]) # 截取x序列的下一个token序列作为目标y\n",
        "    return x, y # 返回输入批次x和目标批次y\n",
        "\n",
        "# --- 主程序执行部分 ---\n",
        "\n",
        "tokenizer = Tokenizer(text) # 实例化Tokenizer\n",
        "vocab_size = tokenizer.vocab_size # 获取词汇表大小\n",
        "tokens = tokenizer.encode(text) # 将整个文本编码为整数ID序列\n",
        "\n",
        "model = BigramLanguageModel(vocab_size) # 实例化BigramLanguageModel\n",
        "\n",
        "# “训练”过程：统计transition矩阵\n",
        "print(\"开始“训练”（统计频率）...\")\n",
        "start_time = time.time() # 记录开始时间\n",
        "for iter in range(max_iters): # 循环 max_iters 次来模拟训练迭代\n",
        "    # 每次迭代随机获取一批数据x和y。\n",
        "    # x是输入序列批次，y是对应的目标序列批次（即x中每个token的下一个token）。\n",
        "    x_batch, y_batch = get_batch(tokens, batch_size, block_size)\n",
        "\n",
        "    # 遍历批次中的每个序列和序列中的每个token，更新transition矩阵。\n",
        "    # 注意：这里我们没有使用模型的forward方法，而是直接修改了模型的内部参数(transition)。\n",
        "    # 这模拟了最简单的“学习”过程：观察数据并更新统计信息。\n",
        "    for i in range(len(x_batch)): # 遍历批次中的每个序列\n",
        "        for j in range(len(x_batch[i])): # 遍历序列中的每个token\n",
        "            current_token_id = x_batch[i][j] # 获取当前输入token的ID\n",
        "            next_token_id = y_batch[i][j] # 获取对应的目标token的ID\n",
        "            model.transition[current_token_id][next_token_id] += 1 # 统计频率\n",
        "\n",
        "if max_iters > 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"“训练”完成，耗时: {elapsed_time:.2f} 秒\")\n",
        "\n",
        "# 推理过程：生成文本\n",
        "print(\"\\n开始推理（生成文本）...\")\n",
        "# 将初始prompt编码为整数ID列表，并转换为批次形式 (List[List[int]])\n",
        "prompt_tokens = [tokenizer.encode(prompt) for prompt in prompts]\n",
        "# 调用模型的generate方法，生成新的token序列\n",
        "result = model.generate(prompt_tokens, max_new_token)\n",
        "\n",
        "# 解码并打印生成的结果\n",
        "print(\"\\n生成结果：\")\n",
        "for tokens_list in result:\n",
        "    print(tokenizer.decode(tokens_list)) # 将生成的token ID列表解码为字符串\n",
        "    print('-'*10) # 分隔符\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIxs5hkljIxg",
        "outputId": "c7dc3aa6-d16d-4157-ef70-9c9927bbbb87"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始“训练”（统计频率）...\n",
            "“训练”完成，耗时: 2.51 秒\n",
            "\n",
            "开始推理（生成文本）...\n",
            "\n",
            "生成结果：\n",
            "春江红紫霄效颦。\n",
            "\n",
            "怎。\n",
            "兰修月。\n",
            "两个事对西风酒伴寄我登临，看雪惊起步，总不与泪满南园春来。\n",
            "最关上阅。\n",
            "信断，名姝，夜正坐认旧武仙 朱弦。\n",
            "\n",
            "岁，回。\n",
            "\n",
            "\n",
            "看一丝竹。\n",
            "愿皇受风，当。\n",
            "\n",
            "妆一笑时，不堪\n",
            "----------\n",
            "往事多闲田舍、十三楚珪\n",
            "酒困不须紫芝兰花痕皱，青步虹。\n",
            "暗殿人物华高层轩者，临江渌池塘。\n",
            "三峡。\n",
            "天、彩霞冠\n",
            "燕翻云垂杨、一声羌笛罢瑶觥船窗幽园春生阵。\n",
            "长桥。\n",
            "无恙，中有心期。\n",
            "\n",
            "开处。\n",
            "燕姹绿遍，烂□\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DKohdqYjI2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YVTRQa19jI6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AYG1rnVAjI9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch_5min.py\n",
        "import torch # 导入PyTorch库\n",
        "from torch import nn # 从torch导入神经网络模块，nn包含了各种神经网络层（如线性层、卷积层等）\n",
        "from torch.nn import functional as F # 导入神经网络函数模块，F包含了激活函数、损失函数等无状态操作\n",
        "\n",
        "torch.manual_seed(42) # 设置PyTorch的随机数种子，确保每次运行结果一致。\n",
        "\n",
        "# 判断环境中是否有GPU（CUDA）或MPS（Apple Silicon GPU）可用，并选择最快的设备。\n",
        "# 'cuda' 用于NVIDIA GPU\n",
        "# 'mps' 用于Apple Silicon Mac的Metal Performance Shaders\n",
        "# 'cpu' 用于CPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu'\n",
        "print(f\"Using {device} device\") # 打印当前使用的设备\n",
        "\n",
        "# 1. 创建tensor演示\n",
        "# torch.tensor() 用于创建张量。张量是PyTorch中数据的基本单位，类似于NumPy数组，但可以在GPU上加速。\n",
        "x = torch.tensor([1.0, 2.0, 3.0]) # 创建一个一维张量x\n",
        "y = torch.tensor([2.0, 4.0, 6.0]) # 创建一个一维张量y\n",
        "\n",
        "# 2. 基本运算演示\n",
        "print(x + y) # 张量逐元素加法: tensor([3., 6., 9.])\n",
        "print(x * y) # 张量逐元素乘法 (点乘): tensor([2., 8., 18.])\n",
        "print(torch.matmul(x, y)) # 矩阵乘法（对于一维向量，这相当于点积）: tensor(28.) (1*2 + 2*4 + 3*6 = 2 + 8 + 18 = 28)\n",
        "print(x @ y) # 另一种矩阵乘法的写法（Python 3.5+ 支持的运算符）: tensor(28.)\n",
        "print(x.shape) # 获取张量的形状 (维度信息): torch.Size([3])\n",
        "\n",
        "# 3. 定义模型：一个简单的线性网络\n",
        "# 在PyTorch中，模型通常继承自nn.Module。\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        # 构造函数，初始化模型层。\n",
        "        super().__init__() # 调用父类nn.Module的构造函数。\n",
        "        # 定义一个线性层 (全连接层)。\n",
        "        # nn.Linear(in_features, out_features) 表示输入维度为in_features，输出维度为out_features。\n",
        "        # 对于 y = wx + b 形式的线性回归，输入x是一个特征，输出y是一个预测值，所以都是1。\n",
        "        self.linear = nn.Linear(1, 1) # 输入维度=1，输出维度=1\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 定义模型的前向传播逻辑。当模型被调用时（例如 model(x)），这个方法会被执行。\n",
        "        return self.linear(x) # 将输入x通过线性层进行计算并返回结果。\n",
        "\n",
        "# 4. 生成训练数据\n",
        "# 真实关系: y = 2x + 1，我们希望模型学习到这个关系。\n",
        "x_train = torch.rand(100, 1) * 10 # 生成100个0到10之间的随机数作为输入x。形状 (100, 1)。\n",
        "# 根据真实关系生成y_train，并添加一些随机噪声，模拟真实世界的数据不完全精确。\n",
        "y_train = 2 * x_train + 1 + torch.randn(100, 1) * 0.1 # 形状 (100, 1)。\n",
        "# 将数据移动到指定的设备（CPU或GPU），以便在相应设备上进行计算。\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "\n",
        "# 5. 创建模型、优化器和损失函数\n",
        "model = SimpleNet().to(device) # 实例化模型，并将其移动到指定设备。\n",
        "# 定义优化器。优化器负责根据损失函数的梯度来更新模型的参数。\n",
        "# torch.optim.SGD (Stochastic Gradient Descent) 是一种常见的优化算法。\n",
        "# model.parameters() 获取模型中所有可训练的参数（w和b）。\n",
        "# lr (learning rate) 是学习率，控制每次参数更新的步长。\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "# 定义损失函数。损失函数衡量模型预测值与真实值之间的差异。\n",
        "# nn.MSELoss (Mean Squared Error Loss) 均方误差，常用于回归任务。\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# 6. 训练循环\n",
        "epochs = 5000 # 定义训练的轮次。\n",
        "\n",
        "print(\"\\n训练开始...\")\n",
        "for epoch in range(epochs):\n",
        "    # 前向传播：模型根据当前参数对输入x_train进行预测，得到y_pred。\n",
        "    y_pred = model(x_train)\n",
        "\n",
        "    # 计算损失：使用损失函数衡量y_pred和真实值y_train之间的差距。\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # 反向传播：\n",
        "    optimizer.zero_grad() # 清除之前计算的梯度。在PyTorch中，梯度会累积，所以每次反向传播前需要清零。\n",
        "    loss.backward() # 执行反向传播，计算损失函数对模型所有可训练参数的梯度。\n",
        "    optimizer.step() # 根据计算出的梯度和学习率，更新模型的参数（w和b）。这就是“梯度下降”的核心步骤。\n",
        "\n",
        "    # 每100个epoch打印一次训练状态。\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        # model.linear.weight.item() 和 model.linear.bias.item() 获取线性层的权重w和偏置b的当前值。\n",
        "        w = model.linear.weight.item()\n",
        "        b = model.linear.bias.item()\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, w: {w:.2f}, b: {b:.2f}')\n",
        "\n",
        "# 7. 打印最终训练结果\n",
        "w = model.linear.weight.item()\n",
        "b = model.linear.bias.item()\n",
        "print(f'\\n训练完成！')\n",
        "print(f'学习到的函数: y = {w:.2f}x + {b:.2f}')\n",
        "print(f'实际函数: y = 2.00x + 1.00')\n",
        "\n",
        "# 8. 测试模型\n",
        "test_x = torch.tensor([[0.0], [5.0], [10.0]]).to(device) # 创建测试数据，并移动到设备。\n",
        "# torch.no_grad() 上下文管理器，表示在这个代码块中不计算梯度。\n",
        "# 在推理或评估阶段，我们不需要计算梯度，这可以节省内存和计算资源。\n",
        "with torch.no_grad():\n",
        "    test_y = model(test_x) # 使用训练好的模型进行预测。\n",
        "    print(\"\\n预测结果：\")\n",
        "    for x_val, y_val in zip(test_x, test_y):\n",
        "        print(f'x = {x_val.item():.1f}, y = {y_val.item():.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXFxqlhxjJAR",
        "outputId": "1d958a62-83d5-484b-9ad4-4e8a46cccbed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "tensor([3., 6., 9.])\n",
            "tensor([ 2.,  8., 18.])\n",
            "tensor(28.)\n",
            "tensor(28.)\n",
            "torch.Size([3])\n",
            "\n",
            "训练开始...\n",
            "Epoch [100/5000], Loss: 0.0988, w: 2.09, b: 0.41\n",
            "Epoch [200/5000], Loss: 0.0420, w: 2.05, b: 0.64\n",
            "Epoch [300/5000], Loss: 0.0202, w: 2.03, b: 0.79\n",
            "Epoch [400/5000], Loss: 0.0118, w: 2.02, b: 0.88\n",
            "Epoch [500/5000], Loss: 0.0086, w: 2.01, b: 0.93\n",
            "Epoch [600/5000], Loss: 0.0074, w: 2.00, b: 0.97\n",
            "Epoch [700/5000], Loss: 0.0069, w: 2.00, b: 0.99\n",
            "Epoch [800/5000], Loss: 0.0067, w: 2.00, b: 1.00\n",
            "Epoch [900/5000], Loss: 0.0067, w: 2.00, b: 1.01\n",
            "Epoch [1000/5000], Loss: 0.0066, w: 2.00, b: 1.01\n",
            "Epoch [1100/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [1200/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [1300/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [1400/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [1500/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [1600/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [1700/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [1800/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [1900/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [2000/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [2100/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [2200/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [2300/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [2400/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [2500/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [2600/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [2700/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [2800/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [2900/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [3000/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [3100/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [3200/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [3300/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [3400/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [3500/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [3600/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [3700/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [3800/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [3900/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [4000/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [4100/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [4200/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [4300/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [4400/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [4500/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [4600/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [4700/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [4800/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [4900/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "Epoch [5000/5000], Loss: 0.0066, w: 2.00, b: 1.02\n",
            "\n",
            "训练完成！\n",
            "学习到的函数: y = 2.00x + 1.02\n",
            "实际函数: y = 2.00x + 1.00\n",
            "\n",
            "预测结果：\n",
            "x = 0.0, y = 1.02\n",
            "x = 5.0, y = 11.00\n",
            "x = 10.0, y = 20.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K8bDIjNolVSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KAPJG8LDlVgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# babygpt_v1.py\n",
        "import torch # 导入PyTorch库\n",
        "import torch.nn as nn # 导入神经网络模块，包含各种层\n",
        "from torch.nn import functional as F # 导入神经网络函数模块，包含激活函数、损失函数等\n",
        "from typing import List # 用于类型注解\n",
        "import time # 用于计时\n",
        "\n",
        "torch.manual_seed(42) # 设置PyTorch的随机数种子，确保结果可复现。\n",
        "\n",
        "# --- 全局配置参数 ---\n",
        "prompts = [\"春江\", \"往事\"] # 用于模型推理的初始输入字符串\n",
        "max_new_token = 100 # 模型生成新token的最大数量\n",
        "max_iters = 5000 # 训练的最大迭代次数（epoch）\n",
        "eval_iters = 100 # 每次评估时，用于计算平均损失的批次数量\n",
        "eval_interval = 200 # 每隔多少次迭代进行一次模型评估并打印损失\n",
        "batch_size = 32 # 每个训练批次中包含的独立序列数量 (B)\n",
        "block_size = 8 # 每个序列的最大长度，即模型考虑的上下文窗口大小 (T)\n",
        "learning_rate = 1e-2 # 优化器的学习率\n",
        "n_embed = 32 # 嵌入层（Embedding Layer）的维度。每个token将被映射到一个n_embed维的向量。\n",
        "tain_data_ratio = 0.9 # 训练数据占总数据集的比例，剩余部分作为验证数据\n",
        "\n",
        "# 设备检测：选择可用的计算设备 (GPU或CPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu'\n",
        "print(f\"Using {device} device\") # 打印当前使用的设备\n",
        "\n",
        "# 读取数据集ci.txt\n",
        "with open('ci.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenizer 类：与simplebigrammodel.py中的实现完全相同，负责文本的编码和解码。\n",
        "class Tokenizer:\n",
        "    def __init__(self, text: str):\n",
        "        self.chars = sorted(list(set(text)))\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.itos = {i: ch for i, ch in enumerate(self.chars)}\n",
        "\n",
        "    def encode(self, s: str) -> List[int]:\n",
        "        return [self.stoi[c] for c in s]\n",
        "\n",
        "    def decode(self, l: List[int]) -> str:\n",
        "        return ''.join([self.itos[i] for i in l])\n",
        "\n",
        "# BabyGPT 类：我们的Bigram语言模型，继承自nn.Module\n",
        "class BabyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size: int, n_embd: int):\n",
        "        # 构造函数，初始化模型层\n",
        "        super().__init__() # 调用父类nn.Module的构造函数\n",
        "        # 1. token_embedding_table (嵌入层):\n",
        "        # nn.Embedding 层是一个查找表，用于将离散的token ID映射到连续的、密集的向量表示。\n",
        "        # vocab_size: 词汇表的大小，即有多少个唯一的token ID。\n",
        "        # n_embd: 每个token ID将被映射到的向量的维度（嵌入维度）。\n",
        "        # 例如，如果token ID是5，它会查找并返回一个长度为n_embd的向量。\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "        # 2. lm_head (语言模型头):\n",
        "        # nn.Linear 层是一个全连接层，用于将中间表示映射回词汇表大小的维度。\n",
        "        # n_embd: 输入维度，来自嵌入层的输出。\n",
        "        # vocab_size: 输出维度，对应词汇表中每个token的预测得分（logits）。\n",
        "        # 模型的最终目标是预测下一个token是词汇表中哪个token，所以输出维度必须是vocab_size。\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        # 模型的前向传播方法。\n",
        "        # idx: 输入的token ID序列批次，形状为 (B, T)。\n",
        "        # targets: 可选参数，真实的下一个token ID序列批次，形状为 (B, T)。在训练时提供，推理时为None。\n",
        "\n",
        "        # 1. 嵌入操作: 将输入的token ID转换为嵌入向量。\n",
        "        # self.token_embedding_table(idx) 会将 idx (B, T) 转换为 tok_emb (B, T, n_embd)。\n",
        "        # 每一个token ID都被其对应的n_embd维向量替换。\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "\n",
        "        # 2. 线性投影: 将嵌入向量投影回词汇表维度，得到logits。\n",
        "        # self.lm_head(tok_emb) 会将 tok_emb (B, T, n_embd) 转换为 logits (B, T, vocab_size)。\n",
        "        # 每个位置的嵌入向量都被线性层处理，输出一个长度为vocab_size的向量，\n",
        "        # 这个向量的每个元素代表了下一个token是对应词汇表中token的“得分”。\n",
        "        logits = self.lm_head(tok_emb) # (B, T, vocab_size)\n",
        "\n",
        "        loss = None\n",
        "        # 如果提供了targets，则计算损失。这通常发生在训练阶段。\n",
        "        if targets is not None:\n",
        "            # 为了计算交叉熵损失，需要对logits和targets的形状进行调整。\n",
        "            # F.cross_entropy 函数的第一个参数 (input) 期望形状为 (N, C)，其中N是样本数，C是类别数。\n",
        "            # 它的第二个参数 (target) 期望形状为 (N)。\n",
        "\n",
        "            B, T, C = logits.shape # 获取logits的批次大小、序列长度和词汇表大小。\n",
        "\n",
        "            # 将logits的形状从 (B, T, C) 变形为 (B*T, C)。\n",
        "            # 这个操作并没有丢失信息，只是改变了张量的逻辑视图，将所有B*T个“预测任务”扁平化。\n",
        "            logits = logits.view(B * T, C)\n",
        "\n",
        "            # 将targets的形状从 (B, T) 变形为 (B*T)。\n",
        "            # 同样是扁平化，将所有B*T个“真实标签”扁平化。\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "            # 计算交叉熵损失。\n",
        "            # 交叉熵损失常用于分类问题，衡量模型预测的概率分布与真实标签之间的差异。\n",
        "            # 对于语言模型，它衡量模型预测的下一个token的概率分布与真实下一个token的“独热编码”之间的差异。\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss # 返回logits和损失。\n",
        "\n",
        "    def generate(self, idx: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
        "        # 序列生成方法。\n",
        "        # idx: 初始的prompt token ID序列批次，形状为 (B, T)。\n",
        "        # max_new_tokens: 需要生成的新token的最大数量。\n",
        "\n",
        "        # 循环生成 max_new_tokens 个新token。\n",
        "        for _ in range(max_new_tokens):\n",
        "            # 在Bigram模型中，我们只关心序列的最后一个token来预测下一个。\n",
        "            # 但在这里，为了与后续更复杂的模型（如Transformer）保持接口一致，\n",
        "            # 我们仍然传入整个idx序列。然而，在实际处理时，我们只取最后一个token的嵌入。\n",
        "            # 这里idx的形状是 (B, T_current)，其中 T_current 会随着生成不断增长。\n",
        "\n",
        "            # 调用模型的forward方法，获取当前序列批次的logits。\n",
        "            # 注意：这里只传idx，因为是推理模式，不需要targets，所以loss会是None。\n",
        "            # logits的形状是 (B, T_current, vocab_size)。\n",
        "            logits, _ = self(idx)\n",
        "\n",
        "            # 提取每个序列中最后一个token的logits。\n",
        "            # logits[:, -1, :] 表示取所有批次 (:) 的最后一个序列位置 (-1) 的所有词汇表维度 (:)。\n",
        "            # 结果形状是 (B, vocab_size)。\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # 使用Softmax函数将logits转换为概率分布。\n",
        "            # F.softmax(input, dim) 会在指定维度上进行Softmax操作。\n",
        "            # dim=-1 表示对最后一个维度（即vocab_size维度）进行Softmax，\n",
        "            # 确保每个token的预测概率和为1。\n",
        "            probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n",
        "\n",
        "            # 根据概率分布随机采样下一个token ID。\n",
        "            # torch.multinomial(input, num_samples) 从input的行中（将其视为概率分布）抽取num_samples个索引。\n",
        "            # 这里input是probs (B, vocab_size)，我们从每个批次的概率分布中采样一个token。\n",
        "            # 结果形状是 (B, 1)。\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # 将新生成的token ID拼接到当前序列的末尾。\n",
        "            # torch.cat((tensor1, tensor2), dim) 沿着指定维度拼接张量。\n",
        "            # dim=1 表示沿着序列长度维度拼接，将idx_next (B, 1) 拼接到idx (B, T_current) 后面。\n",
        "            # 结果形状是 (B, T_current + 1)。\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx # 返回包含生成token的完整序列批次。\n",
        "\n",
        "# --- 数据准备 ---\n",
        "tokenizer = Tokenizer(text) # 实例化分词器\n",
        "vocab_size = tokenizer.vocab_size # 获取词汇表大小\n",
        "# 将整个文本编码为整数ID序列，并转换为PyTorch的long类型张量，然后移动到指定设备。\n",
        "# torch.long 是整数类型，通常用于表示索引或分类标签。\n",
        "raw_data = torch.tensor(tokenizer.encode(text), dtype=torch.long).to(device)\n",
        "\n",
        "# 划分训练集和验证集\n",
        "n = int(tain_data_ratio * len(raw_data)) # 计算训练集的大小\n",
        "data = {'train': raw_data[:n], 'val': raw_data[n:]} # 划分数据\n",
        "\n",
        "# get_batch 函数：用于获取批次数据，PyTorch版本\n",
        "def get_batch(data_split: torch.Tensor, batch_size: int, block_size: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    '''\n",
        "    从给定的数据分割（训练集或验证集）中随机获取一批数据x和y。\n",
        "    x和y都是PyTorch张量，形状分别为 (B, T) 和 (B, T)。\n",
        "    y中的每个值都是x中对应位置值的下一个值。\n",
        "    '''\n",
        "    # 随机选择 batch_size 个起始索引。\n",
        "    # torch.randint(high, size) 生成 size 形状的张量，其元素在 [0, high) 范围内。\n",
        "    # len(data_split) - block_size 确保有足够的空间截取 block_size 长度的序列。\n",
        "    ix = torch.randint(len(data_split) - block_size, (batch_size,)) # (batch_size,)\n",
        "\n",
        "    # 使用torch.stack将多个序列堆叠成一个批次张量。\n",
        "    # data_split[i:i+block_size] 截取单个序列。\n",
        "    # torch.stack([..., ..., ...]) 将这些序列沿新维度堆叠。\n",
        "    x = torch.stack([data_split[i:i+block_size] for i in ix]) # (B, T)\n",
        "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix]) # (B, T)\n",
        "\n",
        "    # 将批次数据移动到指定设备（如果尚未移动）。\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# estimate_loss 函数：用于在训练和验证集上评估模型损失\n",
        "@torch.no_grad() # 装饰器：表示在此函数中不需要计算梯度，节省内存和计算。\n",
        "def estimate_loss(model: nn.Module, data: dict[str, torch.Tensor], batch_size: int, block_size: int, eval_iters: int) -> dict[str, float]:\n",
        "    '''\n",
        "    计算模型在训练集和验证集上的平均损失。\n",
        "    model: 待评估的模型。\n",
        "    data: 包含'train'和'val'数据的字典。\n",
        "    eval_iters: 用于计算平均损失的批次数量。\n",
        "    '''\n",
        "    out = {}\n",
        "    model.eval() # 将模型设置为评估模式。\n",
        "                  # 在评估模式下，某些层（如Dropout、BatchNorm）的行为会发生改变，例如Dropout会关闭。\n",
        "    for split in ['train', 'val']: # 遍历训练集和验证集\n",
        "        losses = torch.zeros(eval_iters, device=device) # 初始化一个张量来存储每次评估的损失\n",
        "        for k in range(eval_iters): # 循环eval_iters次，获取多个批次来计算平均损失\n",
        "            x, y = get_batch(data[split], batch_size, block_size) # 获取一批数据\n",
        "            _, loss = model(x, y) # 前向传播，计算损失\n",
        "            losses[k] = loss.item() # 存储损失值 (loss.item() 将张量转换为Python标量)\n",
        "        out[split] = losses.mean().item() # 计算平均损失并存储\n",
        "    model.train() # 评估完成后，将模型切换回训练模式。\n",
        "    return out\n",
        "\n",
        "# --- 模型实例化与训练 ---\n",
        "model = BabyGPT(vocab_size, n_embed).to(device) # 实例化模型，并将其移动到指定设备。\n",
        "# 计算模型参数量，用于了解模型大小。\n",
        "# model.parameters() 返回一个迭代器，包含模型所有可训练的参数张量。\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"模型参数量: {num_params}\")\n",
        "\n",
        "# 定义优化器。\n",
        "# torch.optim.AdamW 是一种常用的优化算法，比SGD在许多情况下表现更好。\n",
        "# model.parameters() 获取模型所有可训练的参数。\n",
        "# lr=learning_rate 设置学习率。\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"\\n开始训练...\")\n",
        "start_time = time.time() # 记录训练开始时间\n",
        "tokens_processed = 0 # 统计已处理的token数量\n",
        "\n",
        "for iter in range(max_iters): # 训练循环\n",
        "    # 每隔eval_interval次迭代，进行一次评估并打印损失。\n",
        "    if iter % eval_interval == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        tokens_per_sec = tokens_processed / elapsed if elapsed > 0 else 0 # 计算每秒处理的token数\n",
        "        losses = estimate_loss(model, data, batch_size, block_size, eval_iters) # 评估损失\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, speed: {tokens_per_sec:.2f} tokens/sec\")\n",
        "\n",
        "    # 获取一个训练批次的数据\n",
        "    x, y = get_batch(data['train'], batch_size, block_size)\n",
        "\n",
        "    # 前向传播：计算模型的logits和损失。\n",
        "    logits, loss = model(x, y)\n",
        "\n",
        "    # 反向传播与参数更新：\n",
        "    optimizer.zero_grad(set_to_none=True) # 清除旧梯度，set_to_none=True更高效地释放内存\n",
        "    loss.backward() # 计算损失对模型参数的梯度\n",
        "    optimizer.step() # 根据梯度更新模型参数\n",
        "\n",
        "    tokens_processed += batch_size * block_size # 更新已处理token计数\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"\\n训练完成！总耗时: {elapsed_time:.2f} 秒\")\n",
        "\n",
        "# --- 模型推理 (文本生成) ---\n",
        "print(\"\\n开始推理（生成文本）...\")\n",
        "# 将prompt字符串编码为token ID，转换为PyTorch张量，并堆叠成批次。\n",
        "# prompts 是一个字符串列表，需要先encode成List[int]，再转换为torch.tensor。\n",
        "# torch.stack([..., ..., ...]) 将多个一维张量堆叠成一个二维张量 (B, T)。\n",
        "prompt_tokens = torch.stack([torch.tensor(tokenizer.encode(p), dtype=torch.long).to(device) for p in prompts])\n",
        "\n",
        "# 调用模型的generate方法生成新token。\n",
        "# model.eval() 确保在推理时模型处于评估模式（例如禁用Dropout）。\n",
        "model.eval()\n",
        "result = model.generate(prompt_tokens, max_new_token)\n",
        "\n",
        "# 解码并打印生成的结果\n",
        "print(\"\\n生成结果：\")\n",
        "for tokens_list_tensor in result:\n",
        "    # tokens_list_tensor 是一个PyTorch张量，需要先转换为Python列表 (tolist()) 再解码。\n",
        "    print(tokenizer.decode(tokens_list_tensor.tolist()))\n",
        "    print('-'*10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XxOEa_hlVyY",
        "outputId": "67b771ea-1118-436d-b655-b0241debf6c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "模型参数量: 399620\n",
            "\n",
            "开始训练...\n",
            "step 0: train loss 8.9993, val loss 8.9955, speed: 0.00 tokens/sec\n",
            "step 200: train loss 5.8397, val loss 5.9896, speed: 3268.90 tokens/sec\n",
            "step 400: train loss 5.5626, val loss 5.7625, speed: 4519.06 tokens/sec\n",
            "step 600: train loss 5.4655, val loss 5.7280, speed: 5346.57 tokens/sec\n",
            "step 800: train loss 5.3833, val loss 5.5987, speed: 5761.34 tokens/sec\n",
            "step 1000: train loss 5.3514, val loss 5.5978, speed: 6201.94 tokens/sec\n",
            "step 1200: train loss 5.2673, val loss 5.5706, speed: 6426.90 tokens/sec\n",
            "step 1400: train loss 5.2301, val loss 5.5221, speed: 6653.16 tokens/sec\n",
            "step 1600: train loss 5.2357, val loss 5.4799, speed: 6822.79 tokens/sec\n",
            "step 1800: train loss 5.2243, val loss 5.4836, speed: 6936.85 tokens/sec\n",
            "step 2000: train loss 5.1848, val loss 5.4434, speed: 7078.50 tokens/sec\n",
            "step 2200: train loss 5.1590, val loss 5.3750, speed: 7124.70 tokens/sec\n",
            "step 2400: train loss 5.1487, val loss 5.4217, speed: 7233.93 tokens/sec\n",
            "step 2600: train loss 5.0826, val loss 5.3975, speed: 7149.95 tokens/sec\n",
            "step 2800: train loss 5.0981, val loss 5.3775, speed: 7238.56 tokens/sec\n",
            "step 3000: train loss 5.0915, val loss 5.3750, speed: 7260.48 tokens/sec\n",
            "step 3200: train loss 5.0558, val loss 5.3702, speed: 6993.31 tokens/sec\n",
            "step 3400: train loss 5.0571, val loss 5.3815, speed: 7042.61 tokens/sec\n",
            "step 3600: train loss 5.0411, val loss 5.3580, speed: 7012.59 tokens/sec\n",
            "step 3800: train loss 5.0329, val loss 5.3423, speed: 7086.85 tokens/sec\n",
            "step 4000: train loss 5.0089, val loss 5.3004, speed: 7099.61 tokens/sec\n",
            "step 4200: train loss 5.0492, val loss 5.2781, speed: 7160.49 tokens/sec\n",
            "step 4400: train loss 5.0225, val loss 5.3111, speed: 7172.54 tokens/sec\n",
            "step 4600: train loss 5.0243, val loss 5.3003, speed: 7229.14 tokens/sec\n",
            "step 4800: train loss 5.0480, val loss 5.3312, speed: 7236.88 tokens/sec\n",
            "\n",
            "训练完成！总耗时: 175.58 秒\n",
            "\n",
            "开始推理（生成文本）...\n",
            "\n",
            "生成结果：\n",
            "春江里劝檀首五云微拥绣軿。\n",
            "但重记得，瘦巧。\n",
            "念君曾为途灭便久，客思草青青螺连天暮。\n",
            "待惺惺版纤注。\n",
            "满江沙架，红尘起也道是民\n",
            "雅。\n",
            "谁难惜凄气犹在落月殿三一箭锦巾大相为得一枕何梦，梦成脑意，云阳溪东坡贵\n",
            "----------\n",
            "往事。\n",
            "特地华\n",
            "雪，一梦言金门 晁补氏\n",
            "州 陈著\n",
            "休系，移约。\n",
            "料顾。\n",
            "在，伴真\n",
            "辉明月正气，断时节不比李纲\n",
            "鱼窃香聚红小风。\n",
            "还辞庭梅花面觞灯月，不遣梅掩屏山外，消邯郸香节。\n",
            "醉。\n",
            "新倦契，上林表坞，流\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nLods3aNlVIE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}